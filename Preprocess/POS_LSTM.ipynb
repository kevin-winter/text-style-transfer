{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\kevin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import wordfreq\n",
    "import markovify\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from functools import reduce\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from keras.layers import LSTM, Bidirectional, Embedding, Input, Concatenate, Reshape, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from networkx import shortest_path_length, from_dict_of_dicts, shortest_path\n",
    "\n",
    "from transformers import *\n",
    "from helper import *\n",
    "from corpus_helper import CorpusStreamer, CorpusFileHandler\n",
    "\n",
    "sp_parser = spacy.load(\"en\")\n",
    "configure_logging('markov.log', logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starts_with_vowel(word):\n",
    "    return word.lower()[0] in [\"a\",\"e\",\"i\",\"o\",\"u\"]\n",
    "\n",
    "def lexical_freq(word):\n",
    "    return wordfreq.zipf_frequency(word, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = lambda x: (x.tag_, #category\n",
    "                     x.is_stop, #starts_with_vowel(x.text), #bool\n",
    "                     x.i - x.sent.start, x.sent.end - x.i, len(x), lexical_freq(x.text)) #number\n",
    "\n",
    "class TextParser(Pipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__([\n",
    "            (\"TextCleaner\",TextCleaner()),\n",
    "            (\"TextFeatureExtractor\", TextFeatureExtractor(mapping))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncoder(Pipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__([\n",
    "            (\"_union\", FeatureUnion([\n",
    "                (\"_discrete_features\", Pipeline([\n",
    "                    (\"TypeSelector_Discrete\", TypeSelector([\"object\", \"category\"])),\n",
    "                    (\"OneHotEncoder\", OrdinalEncoder())\n",
    "                ])),\n",
    "                (\"_boolean_features\", Pipeline([\n",
    "                    (\"TypeSelector_Continuous\", TypeSelector([\"bool\"])),\n",
    "                    (\"MinMaxScaler\", MinMaxScaler(feature_range=[0,1]))\n",
    "                ])),\n",
    "                (\"_continous_features\", Pipeline([\n",
    "                    (\"TypeSelector_Continuous\", TypeSelector([\"number\"])),\n",
    "                    (\"MinMaxScaler\", MinMaxScaler(feature_range=[0,1]))\n",
    "                ]))\n",
    "            ]))\n",
    "        ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGenerator(TimeseriesGenerator):\n",
    "    def __getitem__(self,index):\n",
    "        return self.transformed_out(super().__getitem__(index))\n",
    "    \n",
    "    def transformed_out(self, out):\n",
    "        x, y = out\n",
    "        \n",
    "        return \\\n",
    "            {\"pos_in\": x[:,:,0:1], \n",
    "             \"rest_in\": x[:,:,1:]}, \\\n",
    "            \\\n",
    "            {\"pos_out\": to_categorical(y[:,0:1], num_classes=nr_tags), \n",
    "             \"stop_out\": y[:,1:2], \n",
    "             \"freq_out\": y[:,-2:-1], \n",
    "             \"tf_out\": y[:,-1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0cd31e4f901a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "features = TextParser().fit_transform(text)\n",
    "encoder = FeatureEncoder()\n",
    "data = encoder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = encoder.steps[0][1].transformer_list[0][1].steps[1][1].categories_[0]\n",
    "\n",
    "nr_tags = len(tags)\n",
    "tag_emb_size = int(np.ceil(nr_tags**(1/4)))\n",
    "seq_input_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in = Input((seq_input_len,1,), name=\"pos_in\")\n",
    "\n",
    "emb = Embedding(nr_tags, tag_emb_size)(pos_in)\n",
    "emb = Reshape((seq_input_len, tag_emb_size))(emb)\n",
    "rest_in = Input((seq_input_len, data.shape[1] - 1, ), name=\"rest_in\")\n",
    "\n",
    "x = Concatenate()([emb,rest_in])\n",
    "x = Bidirectional(LSTM(100))(x)\n",
    "x = Dense(50)(x)\n",
    "\n",
    "pos_out = Dense(nr_tags, activation=\"softmax\", name=\"pos_out\")(x)\n",
    "stop_out = Dense(1, activation=\"sigmoid\", name=\"stop_out\")(x)\n",
    "freq_out = Dense(1, activation=\"relu\", name=\"freq_out\")(x)\n",
    "tf_out = Dense(1, activation=\"relu\", name=\"tf_out\")(x)\n",
    "\n",
    "model = Model([pos_in, rest_in], [pos_out, stop_out, freq_out, tf_out])\n",
    "model.compile(\"adam\", [\"categorical_crossentropy\", \"binary_crossentropy\", \"mse\", \"mse\"], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = CustomGenerator(data, data, 10, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d582fdae7e540d59e832480f42530e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=10, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5260394de46b4baaa7a26ec734a558ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=3924, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8621ff128bbd4ab7a4d11836653a3a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=3924, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955878e0e9284700b7b08104a23a63b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=3924, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151af3b1b3094ed5a1e438f6ad6a7078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=3924, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa030154e4d45ea87e70ec8765ccfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=3924, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388ebe104c3847c18ba8dcdb707a77ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=3924, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf4f677b53d4017a30d87f6d8f060ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=3924, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87277e2c3c93490fa31f166a69eb753f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=3924, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0a91b5191841b78d1bcfdc2a58a1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=3924, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b0da59290e49658cd17cc2a8ea0b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=3924, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27388913d30>"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(gen, epochs=10, callbacks=[TQDMNotebookCallback(leave_inner=True)], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_on_batch(x)\n",
    "tags[np.argmax(y_pred[0], axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARKOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bining(number, nbins):\n",
    "    return min(int(number / (1 / nbins)), nbins - 1)\n",
    "\n",
    "def mapping(word):\n",
    "    return (word.tag_, \n",
    "            word.is_stop*1,\n",
    "            bining(len(word)/15, 3), \n",
    "            bining(lexical_freq(word.text)/10, 3),\n",
    "            word.dep_, \n",
    "            bining(word.i / word.sent.end, 3))\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, punct_spacing=True):\n",
    "        self.punct_spacing = punct_spacing\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = re.sub(\"(\\r?\\n)+\",\" \", X)\n",
    "        X = re.sub(\"[^A-Za-z,.?! ]\",\"\", X)\n",
    "        if not self.punct_spacing:\n",
    "            X = re.sub(\" ([.!?,])\", \"\\g<1>\", X)\n",
    "        else:\n",
    "            X = re.sub(\"([.!?,])\", \" \\g<1>\", X)\n",
    "        X = re.sub(\" +\",\" \", X)\n",
    "        return X\n",
    "\n",
    "class TextParser(Pipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__([\n",
    "            (\"TextCleaner\",TextCleaner()),\n",
    "            (\"TextFeatureExtractor\", TextFeatureExtractor(mapping))\n",
    "        ])\n",
    "        \n",
    "def to_style_tokens(text):\n",
    "    features = TextParser().fit_transform(text)\n",
    "    tokens_text = \" \".join(features.apply(lambda x: \"_\".join(map(str, x)), axis=1))\n",
    "    return re.sub(\" ([.!?])[^ ]+\", \"\\g<1>\", tokens_text)\n",
    "\n",
    "def pos_emission_prob(folder_path):\n",
    "    corpus = CorpusStreamer(folder_path, False)\n",
    "    counts = defaultdict(Counter)\n",
    "    \n",
    "    for text in corpus:\n",
    "        doc = nlp()(TextCleaner().fit_transform(text))\n",
    "        for w, t in zip(doc, map(lambda x: '_'.join(map(str, x)), map(mapping, doc))):\n",
    "            counts[t][w.orth_.lower()] += 1\n",
    "        \n",
    "    save_folder = os.path.join(folder_path, 'parsed')\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(save_folder, 'emission_probs.pkl'), 'wb') as f:\n",
    "        pkl.dump(counts, f)\n",
    "        \n",
    "    return counts\n",
    "\n",
    "\n",
    "class LowerMarkovifyText(markovify.Text):\n",
    "    def word_split(self, sentence):\n",
    "        return re.split(super().word_split_pattern, sentence.lower())\n",
    "    \n",
    "\n",
    "def pos_markov_chain(folder_path, state_size=2):\n",
    "    corpus = CorpusStreamer(folder_path, False)\n",
    "    saved_chain = None\n",
    "    saved_pos_chain = None\n",
    "    \n",
    "    for i, text in enumerate(corpus):\n",
    "        logging.debug('POS_MARKOV_CHAIN: {: >5} texts parsed'.format(i+1))\n",
    "        chain = LowerMarkovifyText(TextCleaner(False).transform(text), state_size=state_size, retain_original=False)\n",
    "        saved_chain = markovify.combine([saved_chain, chain]) if saved_chain else chain\n",
    "            \n",
    "        pos_chain = markovify.Text(to_style_tokens(text), state_size=state_size, retain_original=False)\n",
    "        saved_pos_chain = markovify.combine([saved_pos_chain, pos_chain]) if saved_pos_chain else pos_chain\n",
    "    \n",
    "    save_folder = os.path.join(folder_path, 'parsed')\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(save_folder, 'word_mm.pkl'), 'w') as f:\n",
    "        f.write(saved_chain.to_json())\n",
    "        \n",
    "    with open(os.path.join(save_folder, 'pos_mm.pkl'), 'w') as f:\n",
    "        f.write(saved_pos_chain.to_json())\n",
    "    \n",
    "    return saved_chain, saved_pos_chain\n",
    "\n",
    "def markov_to_graph(markovchain):\n",
    "    markovmodel = {}\n",
    "\n",
    "    for k,v in markovchain.chain.model.items():\n",
    "        _sum = sum(v.values())\n",
    "        \n",
    "        markovmodel[k[-1]] = {_k: {\"weight\":-np.log(_v/_sum)} for _k, _v in v.items()}\n",
    "\n",
    "    return from_dict_of_dicts(markovmodel)\n",
    "\n",
    "def limit_style_tokens(word, length=4):\n",
    "    return '_'.join(word.split('_')[:length])\n",
    "\n",
    "def find_token_limiting(nodes, word):\n",
    "    np.random.shuffle(nodes)\n",
    "    short_nodes = list(map(limit_style_tokens, nodes))\n",
    "    return nodes[short_nodes.index(limit_style_tokens(word))]\n",
    "\n",
    "def make_sentence_containing(markovchain, words, tokenize=True, strict=False):\n",
    "    g = markov_to_graph(markovchain)\n",
    "    \n",
    "    if tokenize:\n",
    "        words = [find_token_limiting(list(g.nodes), to_style_tokens(word)) for word in words]\n",
    "        \n",
    "    if not strict:\n",
    "        np.random.shuffle(words)\n",
    "        \n",
    "    tokens = ['___BEGIN__'] + words + ['___END__']\n",
    "    sentence = []\n",
    "    for i in range(len(tokens) - 1):\n",
    "        path = shortest_path(g, tokens[i], tokens[i+1], weight=\"weight\")[1:]\n",
    "        sentence.extend(path)\n",
    "    \n",
    "    return ' '.join(sentence[:-1])\n",
    "\n",
    "def find_sentence_containing(markovchain, words, tokenize=True, max_tries=10000, max_words=25):\n",
    "    if tokenize:\n",
    "        words = list(map(limit_style_tokens, map(to_style_tokens, words)))\n",
    "        \n",
    "    for i in range(max_tries):\n",
    "        sent = markovchain.make_sentence(tries=100, max_words=max_words)\n",
    "        tmp_sent = sent\n",
    "        \n",
    "        for word in words:\n",
    "            if word in tmp_sent:\n",
    "                tmp_sent = tmp_sent.replace(word, '', 1)\n",
    "            else:\n",
    "                sent = ''\n",
    "                break\n",
    "        \n",
    "        if sent:\n",
    "            print('Finding a sentence took {} tries.'.format(i))\n",
    "            return sent\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "def safe_find(array, item):\n",
    "    try:\n",
    "        return array.index(item)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def log_normalize_dict(d):\n",
    "    if isinstance(d, list):\n",
    "        d = {k: 1 for k in d}\n",
    "\n",
    "    norm_sum = sum(d.values())\n",
    "    return {k: np.log(v / norm_sum) for k, v in d.items()}\n",
    "\n",
    "def normalize_dict(d):\n",
    "    if isinstance(d, list):\n",
    "        d = {k: 1 for k in d}\n",
    "\n",
    "    norm_sum = sum(d.values())\n",
    "    return {k: v / norm_sum for k, v in d.items()}\n",
    "\n",
    "def len_norm(n, t, weight=.7):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    \n",
    "    norm = n / (1 / n - weight * (n - t)**2 / (1 - t)**2)\n",
    "#     norm = (5 + n) ** weight / (5 + 1) ** weight\n",
    "#     norm = n ** weight if n > 5 else n\n",
    "    return norm\n",
    "\n",
    "def eos_norm(n, t, weight=0.2, log=True):\n",
    "    norm = (n+1) / t\n",
    "    return weight * np.log(norm) if log else norm\n",
    "\n",
    "def beam_search(word_mm, pos_mm, context_words, beam_size=5, smoothing_prob=1e-6, \n",
    "                word_trans_weight=.5, emission_weight=.5, context_weight=.05, eos_norm_weight=.2, len_norm_weight=.7, \n",
    "                begin_token='___BEGIN__', end_token='___END__',\n",
    "                variable_length=True, max_length=30):\n",
    "    \n",
    "    weights = [word_trans_weight, emission_weight, context_weight]\n",
    "    word_trans_weight, emission_weight, context_weight = np.array(weights) / sum(weights)\n",
    "    \n",
    "    pos_sent = find_sentence_containing(pos_mm, context_words, max_words=max_length).split()\n",
    "    n_t = len(pos_sent)\n",
    "    print(n_t)\n",
    "#     pos_sent = pos_mm.make_sentence(max_words=max_length).split()\n",
    "    \n",
    "    queue = {tuple(): 0}\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        cur_tag = pos_sent[i] if i < len(pos_sent) else ''\n",
    "        layer_candidates = queue if variable_length and i != 0 else {}\n",
    "        \n",
    "        for prev_words, prev_score in queue.items():\n",
    "#             cur_tag = pos_sent[len(prev_words)] if len(prev_words) < len(pos_sent) else ''\n",
    "            if (prev_words and prev_words[-1] == end_token) or len(prev_words) == max_length:\n",
    "                if not variable_length:\n",
    "                    layer_candidates = {**layer_candidates, **{prev_words: prev_score}}\n",
    "                continue\n",
    "                 \n",
    "            n = len(prev_words)\n",
    "            prev_state = [begin_token] * (word_mm.chain.state_size - n) + list(prev_words[-word_mm.chain.state_size:])\n",
    "            \n",
    "#             transition_candidates = log_normalize_dict(word_mm.chain.model.get(tuple(prev_state), {}))\n",
    "#             emission_candidates = log_normalize_dict(emission_probs[cur_tag])\n",
    "#             context_candidates = log_normalize_dict(context_words)\n",
    "\n",
    "#             merged_log_probs = pd.DataFrame([transition_candidates, emission_candidates, context_candidates])\\\n",
    "#                                 .fillna(np.log(smoothing_prob))\\\n",
    "#                                 .apply(lambda x: sum(x * [word_trans_weight, emission_weight, context_weight]))\n",
    "            \n",
    "#             merged_log_probs[end_token] = merged_log_probs.get(end_token, np.log(smoothing_prob)) + eos_norm(len(prev_words), len(pos_sent), eos_norm_weight)\n",
    "            \n",
    "#             reduction_words = set(prev_words[-3:]) or (set(prev_words) and set(context_words))\n",
    "#             for word in reduction_words:\n",
    "#                 merged_log_probs[word] = np.log(smoothing_prob)\n",
    "            \n",
    "#             merged_log_probs -= reduce(np.logaddexp, merged_log_probs)\n",
    "            \n",
    "            transition_candidates = normalize_dict(word_mm.chain.model.get(tuple(prev_state), {}))\n",
    "            emission_candidates = normalize_dict(emission_probs[cur_tag])\n",
    "            context_candidates = normalize_dict(context_words)\n",
    "\n",
    "            merged_probs = pd.DataFrame([transition_candidates, emission_candidates, context_candidates])\\\n",
    "                                .fillna(smoothing_prob)\\\n",
    "                                .apply(lambda x: sum(x * [word_trans_weight, emission_weight, context_weight]))\n",
    "            \n",
    "            merged_probs[end_token] = merged_probs.get(end_token, smoothing_prob) + eos_norm(n, n_t, eos_norm_weight, False)\n",
    "            \n",
    "            reduction_words = set(prev_words[-3:]) or (set(prev_words) and set(context_words))\n",
    "            for word in reduction_words:\n",
    "                merged_probs[word] = smoothing_prob if word in merged_probs else 0\n",
    "            \n",
    "            merged_probs /= sum(merged_probs)\n",
    "            merged_log_probs = np.log(merged_probs)\n",
    "            \n",
    "            selected_candidates = merged_log_probs.nlargest(beam_size)\n",
    "            selected_candidates = {tuple(prev_words) + (word,): (prev_score * len_norm(n, n_t, len_norm_weight) + score) / len_norm(n + 1, n_t, len_norm_weight)\n",
    "                                   for word, score in selected_candidates.items()}\n",
    "            layer_candidates = {**layer_candidates, **selected_candidates}\n",
    "        \n",
    "        old_queue = queue\n",
    "        queue = {k: layer_candidates[k] for k in sorted(layer_candidates, key=layer_candidates.get, reverse=True)[:beam_size]}\n",
    "        \n",
    "        if set(queue.keys()) == set(old_queue.keys()):\n",
    "            break\n",
    "            \n",
    "#         for words, score in queue.items():\n",
    "#             print(' '.join(words), score)\n",
    "#         print()\n",
    "        i += 1\n",
    "    \n",
    "    results = dict(filter(lambda x: end_token in x[0], queue.items()))\n",
    "    if len(results) == 0:\n",
    "        results = queue\n",
    "        best = max(results, key=results.get)\n",
    "        sent = ' '.join(best)\n",
    "    else:\n",
    "        best = max(results, key=results.get)\n",
    "        sent = ' '.join(best[:safe_find(best, end_token)])\n",
    "        \n",
    "    print(sent)\n",
    "    return sent, queue[best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../DataAcquisition/data/dickens\n",
      "2019-04-16 21:47:39,117:INFO:CorpusStreamer: Loading 63 files.\n",
      "2019-04-16 21:48:16,490:INFO:  6% parsed\n",
      "2019-04-16 21:48:49,499:INFO: 12% parsed\n",
      "2019-04-16 21:52:39,025:INFO: 19% parsed\n",
      "2019-04-16 21:57:22,637:INFO: 25% parsed\n",
      "2019-04-16 22:00:59,272:INFO: 31% parsed\n",
      "2019-04-16 22:07:22,788:INFO: 38% parsed\n",
      "2019-04-16 22:07:54,290:INFO: 44% parsed\n",
      "2019-04-16 22:18:57,738:INFO: 50% parsed\n",
      "2019-04-16 22:20:09,884:INFO: 57% parsed\n",
      "2019-04-16 22:24:28,413:INFO: 63% parsed\n",
      "2019-04-16 22:25:17,335:INFO: 69% parsed\n",
      "2019-04-16 22:26:17,288:INFO: 76% parsed\n",
      "2019-04-16 22:27:14,821:INFO: 82% parsed\n",
      "2019-04-16 22:35:06,931:INFO: 88% parsed\n",
      "2019-04-16 22:43:36,353:INFO: 95% parsed\n",
      "2019-04-16 22:44:03,431:INFO:100% parsed\n",
      "2019-04-16 22:44:18,306:INFO:CorpusStreamer: Loading 63 files.\n",
      "2019-04-16 22:45:00,025:INFO:  6% parsed\n",
      "2019-04-16 22:45:32,697:INFO: 12% parsed\n",
      "2019-04-16 22:48:01,181:INFO: 19% parsed\n",
      "2019-04-16 22:50:21,822:INFO: 25% parsed\n",
      "2019-04-16 22:51:20,006:INFO: 31% parsed\n",
      "2019-04-16 22:53:36,692:INFO: 38% parsed\n",
      "2019-04-16 22:53:51,056:INFO: 44% parsed\n",
      "2019-04-16 22:56:37,429:INFO: 50% parsed\n",
      "2019-04-16 22:57:11,900:INFO: 57% parsed\n",
      "2019-04-16 22:58:13,288:INFO: 63% parsed\n",
      "2019-04-16 22:58:38,650:INFO: 69% parsed\n",
      "2019-04-16 22:59:03,974:INFO: 76% parsed\n",
      "2019-04-16 22:59:56,555:INFO: 82% parsed\n",
      "2019-04-16 23:01:12,181:INFO: 88% parsed\n",
      "2019-04-16 23:02:41,319:INFO: 95% parsed\n",
      "2019-04-16 23:02:46,391:INFO:100% parsed\n",
      "../DataAcquisition/data/verne\n",
      "2019-04-16 23:02:46,492:INFO:CorpusStreamer: Loading 36 files.\n",
      "2019-04-16 23:03:17,801:INFO:  5% parsed\n",
      "2019-04-16 23:03:57,119:INFO: 11% parsed\n",
      "2019-04-16 23:04:43,991:INFO: 16% parsed\n",
      "2019-04-16 23:05:12,087:INFO: 22% parsed\n",
      "2019-04-16 23:06:14,603:INFO: 27% parsed\n",
      "2019-04-16 23:07:55,459:INFO: 33% parsed\n",
      "2019-04-16 23:09:19,296:INFO: 38% parsed\n",
      "2019-04-16 23:10:55,134:INFO: 44% parsed\n",
      "2019-04-16 23:12:43,135:INFO: 50% parsed\n",
      "2019-04-16 23:14:58,330:INFO: 55% parsed\n",
      "2019-04-16 23:15:40,621:INFO: 61% parsed\n",
      "2019-04-16 23:17:34,317:INFO: 66% parsed\n",
      "2019-04-16 23:19:52,866:INFO: 72% parsed\n",
      "2019-04-16 23:22:34,150:INFO: 77% parsed\n",
      "2019-04-16 23:25:22,928:INFO: 83% parsed\n",
      "2019-04-16 23:28:09,306:INFO: 88% parsed\n",
      "2019-04-16 23:31:21,525:INFO: 94% parsed\n",
      "2019-04-16 23:34:53,194:INFO:100% parsed\n",
      "2019-04-16 23:35:03,384:INFO:CorpusStreamer: Loading 36 files.\n",
      "2019-04-16 23:35:33,103:INFO:  5% parsed\n",
      "2019-04-16 23:36:06,646:INFO: 11% parsed\n",
      "2019-04-16 23:36:30,539:INFO: 16% parsed\n",
      "2019-04-16 23:36:44,382:INFO: 22% parsed\n",
      "2019-04-16 23:37:42,197:INFO: 27% parsed\n",
      "2019-04-16 23:38:37,506:INFO: 33% parsed\n",
      "2019-04-16 23:39:05,947:INFO: 38% parsed\n",
      "2019-04-16 23:40:02,366:INFO: 44% parsed\n",
      "2019-04-16 23:40:32,900:INFO: 50% parsed\n",
      "2019-04-16 23:41:09,902:INFO: 55% parsed\n",
      "2019-04-16 23:41:19,507:INFO: 61% parsed\n",
      "2019-04-16 23:41:45,305:INFO: 66% parsed\n",
      "2019-04-16 23:42:13,381:INFO: 72% parsed\n",
      "2019-04-16 23:42:53,963:INFO: 77% parsed\n",
      "2019-04-16 23:43:15,290:INFO: 83% parsed\n",
      "2019-04-16 23:43:32,681:INFO: 88% parsed\n",
      "2019-04-16 23:44:06,553:INFO: 94% parsed\n",
      "2019-04-16 23:44:47,900:INFO:100% parsed\n",
      "../DataAcquisition/data/wells\n",
      "2019-04-16 23:44:48,136:INFO:CorpusStreamer: Loading 44 files.\n",
      "2019-04-16 23:45:30,582:INFO:  6% parsed\n",
      "2019-04-16 23:45:59,335:INFO: 13% parsed\n",
      "2019-04-16 23:46:19,663:INFO: 20% parsed\n",
      "2019-04-16 23:46:59,479:INFO: 27% parsed\n",
      "2019-04-16 23:48:03,212:INFO: 34% parsed\n",
      "2019-04-16 23:48:17,366:INFO: 40% parsed\n",
      "2019-04-16 23:48:51,915:INFO: 47% parsed\n",
      "2019-04-16 23:51:15,978:INFO: 54% parsed\n",
      "2019-04-16 23:55:23,822:INFO: 61% parsed\n",
      "2019-04-16 23:59:16,355:INFO: 68% parsed\n",
      "2019-04-17 00:04:33,287:INFO: 75% parsed\n",
      "2019-04-17 00:05:53,540:INFO: 81% parsed\n",
      "2019-04-17 00:07:58,548:INFO: 88% parsed\n",
      "2019-04-17 00:12:33,822:INFO: 95% parsed\n",
      "2019-04-17 00:13:58,912:INFO:100% parsed\n",
      "2019-04-17 00:14:08,681:INFO:CorpusStreamer: Loading 44 files.\n",
      "2019-04-17 00:15:02,634:INFO:  6% parsed\n",
      "2019-04-17 00:15:37,894:INFO: 13% parsed\n",
      "2019-04-17 00:16:01,448:INFO: 20% parsed\n",
      "2019-04-17 00:16:23,335:INFO: 27% parsed\n",
      "2019-04-17 00:17:21,274:INFO: 34% parsed\n",
      "2019-04-17 00:17:36,487:INFO: 40% parsed\n",
      "2019-04-17 00:18:15,275:INFO: 47% parsed\n",
      "2019-04-17 00:19:04,709:INFO: 54% parsed\n",
      "2019-04-17 00:19:39,759:INFO: 61% parsed\n",
      "2019-04-17 00:20:33,260:INFO: 68% parsed\n",
      "2019-04-17 00:21:07,960:INFO: 75% parsed\n",
      "2019-04-17 00:21:43,248:INFO: 81% parsed\n",
      "2019-04-17 00:22:16,944:INFO: 88% parsed\n",
      "2019-04-17 00:23:07,025:INFO: 95% parsed\n",
      "2019-04-17 00:23:33,647:INFO:100% parsed\n"
     ]
    }
   ],
   "source": [
    "folder_path = '../DataAcquisition/data/'\n",
    "for author in os.listdir(folder_path)[2:]:\n",
    "    path = os.path.join(folder_path, author)\n",
    "    print(path)\n",
    "    \n",
    "    chain, pos_chain = pos_markov_chain(path, state_size=3)\n",
    "    emission_probs = pos_emission_prob(path)\n",
    "    #g = markov_to_graph(pos_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thing is of so particular man , those of these world i think i am an mediaeval man of the man of saying from spectacles and the other , which stood so so is the thing and one of the man that he has not called to be right of the man that when of world , the thing had been flung or that there is an thing'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([emission_probs[tag].most_common(1)[0][0] for tag in pos_chain.make_sentence().replace('.', '').split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['war', 'men', 'created']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding a sentence took 891 tries.\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kevin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:237: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i the prince silenced him by a simultaneous movement towards him, each , i have been\n"
     ]
    }
   ],
   "source": [
    "probs = beam_search(chain, pos_chain, emission_probs, words, \n",
    "                    beam_size=10, \n",
    "                    word_trans_weight=1, \n",
    "                    emission_weight=1, \n",
    "                    context_weight=.2, \n",
    "                    eos_norm_weight=0, \n",
    "                    len_norm_weight=.05,\n",
    "                    smoothing_prob=1e-6,\n",
    "                    variable_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = sum((Counter(x) for x in chain.chain.model.values()), Counter())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
