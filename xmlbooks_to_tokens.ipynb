{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(file):\n",
    "    bs_file = BS(file, from_encoding=\"UTF-8\")\n",
    "    return bs_file.find(\"text\").getText()\n",
    "\n",
    "def tokenize(text):\n",
    "    return list(map(word_tokenize, sent_tokenize(text.lower())))\n",
    "\n",
    "def percentile_action(i, N, p, f, *fargs):\n",
    "    if not i%np.ceil(N*p/100) or i==N:\n",
    "        return f(i,N,p,fargs)\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def save_sents(i,N,p,args):\n",
    "    percent = i*100//N\n",
    "    print(\"{:3d}% parsed\".format(percent))\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(\"tokens\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    with open(\"tokens/w2v_tokens_{}.pkl\".format(percent//p), \"wb\") as out:\n",
    "        pkl.dump(args[0], out)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def xml_to_tokens(path):\n",
    "    files = os.listdir(filepath)\n",
    "    nr_files = len(files)\n",
    "    sents = []\n",
    "    \n",
    "    for i, fname in enumerate(files):\n",
    "        with open(path + fname, \"r\", encoding=\"utf8\") as file:\n",
    "            try:\n",
    "                sents += tokenize(get_text(file))\n",
    "            except:\n",
    "                print(\"Could not parse '{}'\".format(fname))\n",
    "\n",
    "        if percentile_action(i+1, nr_files, 10, save_sents, sents):\n",
    "            sents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"C:/Users/kevin/Downloads/GutenTag-master/GutenTag-master/out2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/kevin-winter/text-style-transfer\n",
      " * branch            HEAD       -> FETCH_HEAD\n"
     ]
    }
   ],
   "source": [
    "!git pull https://kevin-winter:@github.com/kevin-winter/text-style-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kevin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from CorpusStreamer import CorpusStreamer\n",
    "from W2V import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenpath = \"C:/Users/kevin/Downloads/TEXT/tokens/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = CorpusStreamer(tokenpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V: Create corpus\n",
      "W2V: Loading: w2v_tokens_1.pkl\n",
      "W2V: Loading: w2v_tokens_10.pkl\n",
      "W2V: Loading: w2v_tokens_2.pkl\n",
      "W2V: Loading: w2v_tokens_3.pkl\n",
      "W2V: Loading: w2v_tokens_4.pkl\n",
      "W2V: Loading: w2v_tokens_5.pkl\n",
      "W2V: Loading: w2v_tokens_6.pkl\n",
      "W2V: Loading: w2v_tokens_7.pkl\n",
      "W2V: Loading: w2v_tokens_8.pkl\n",
      "W2V: Loading: w2v_tokens_9.pkl\n",
      "W2V: Loading: w2v_tokens_1.pkl\n",
      "W2V: Loading: w2v_tokens_10.pkl\n",
      "W2V: Loading: w2v_tokens_2.pkl\n",
      "W2V: Loading: w2v_tokens_3.pkl\n",
      "W2V: Loading: w2v_tokens_4.pkl\n",
      "W2V: Loading: w2v_tokens_5.pkl\n",
      "W2V: Loading: w2v_tokens_6.pkl\n",
      "W2V: Loading: w2v_tokens_7.pkl\n",
      "W2V: Loading: w2v_tokens_8.pkl\n",
      "W2V: Loading: w2v_tokens_9.pkl\n",
      "W2V: Loading: w2v_tokens_1.pkl\n",
      "W2V: Loading: w2v_tokens_10.pkl\n",
      "W2V: Loading: w2v_tokens_2.pkl\n",
      "W2V: Loading: w2v_tokens_3.pkl\n",
      "W2V: Loading: w2v_tokens_4.pkl\n",
      "W2V: Loading: w2v_tokens_5.pkl\n",
      "W2V: Loading: w2v_tokens_6.pkl\n",
      "W2V: Loading: w2v_tokens_7.pkl\n",
      "W2V: Loading: w2v_tokens_8.pkl\n",
      "W2V: Loading: w2v_tokens_9.pkl\n",
      "W2V: Loading: w2v_tokens_1.pkl\n",
      "W2V: Loading: w2v_tokens_10.pkl\n",
      "W2V: Loading: w2v_tokens_2.pkl\n",
      "W2V: Loading: w2v_tokens_3.pkl\n",
      "W2V: Loading: w2v_tokens_4.pkl\n",
      "W2V: Loading: w2v_tokens_5.pkl\n",
      "W2V: Loading: w2v_tokens_6.pkl\n",
      "W2V: Loading: w2v_tokens_7.pkl\n",
      "W2V: Loading: w2v_tokens_8.pkl\n",
      "W2V: Loading: w2v_tokens_9.pkl\n",
      "W2V: Loading: w2v_tokens_1.pkl\n",
      "W2V: Loading: w2v_tokens_10.pkl\n",
      "W2V: Loading: w2v_tokens_2.pkl\n",
      "W2V: Loading: w2v_tokens_3.pkl\n",
      "W2V: Loading: w2v_tokens_4.pkl\n",
      "W2V: Loading: w2v_tokens_5.pkl\n",
      "W2V: Loading: w2v_tokens_6.pkl\n",
      "W2V: Loading: w2v_tokens_7.pkl\n",
      "W2V: Loading: w2v_tokens_8.pkl\n",
      "W2V: Loading: w2v_tokens_9.pkl\n",
      "W2V: Loading: w2v_tokens_1.pkl\n",
      "W2V: Loading: w2v_tokens_10.pkl\n",
      "W2V: Loading: w2v_tokens_2.pkl\n",
      "W2V: Loading: w2v_tokens_3.pkl\n",
      "W2V: Loading: w2v_tokens_4.pkl\n",
      "W2V: Loading: w2v_tokens_5.pkl\n",
      "W2V: Loading: w2v_tokens_6.pkl\n",
      "W2V: Loading: w2v_tokens_7.pkl\n",
      "W2V: Loading: w2v_tokens_8.pkl\n",
      "W2V: Loading: w2v_tokens_9.pkl\n",
      "W2V: Train model\n",
      "W2V: Loading: w2v_tokens_1.pkl\n",
      "W2V: Loading: w2v_tokens_10.pkl\n",
      "W2V: Loading: w2v_tokens_2.pkl\n",
      "W2V: Loading: w2v_tokens_3.pkl\n",
      "W2V: Loading: w2v_tokens_4.pkl\n",
      "W2V: Loading: w2v_tokens_5.pkl\n",
      "W2V: Loading: w2v_tokens_6.pkl\n",
      "W2V: Loading: w2v_tokens_7.pkl\n",
      "W2V: Loading: w2v_tokens_8.pkl\n",
      "W2V: Loading: w2v_tokens_9.pkl\n"
     ]
    }
   ],
   "source": [
    "w2v = trainW2V(stream, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries: 0.243\n",
      "capital-world: 0.049\n",
      "currency: 0.012\n",
      "city-in-state: 0.073\n",
      "family: 0.621\n",
      "gram1-adjective-to-adverb: 0.298\n",
      "gram2-opposite: 0.174\n",
      "gram3-comparative: 0.900\n",
      "gram4-superlative: 0.668\n",
      "gram5-present-participle: 0.563\n",
      "gram6-nationality-adjective: 0.271\n",
      "gram7-past-tense: 0.637\n",
      "gram8-plural: 0.776\n",
      "gram9-plural-verbs: 0.541\n",
      "Total accuracy: 0.348\n"
     ]
    }
   ],
   "source": [
    "for measure, score in testresult_scores.items():\n",
    "        print(\"{}: {:.3f}\".format(measure, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_w2v(w2v, analogies_filepath=\"https://raw.githubusercontent.com/RaRe-Technologies/gensim/develop/gensim/test/test_data/questions-words.txt\"):\n",
    "    testresults = w2v.wv.evaluate_word_analogies(analogies_filepath, case_insensitive=True, restrict_vocab=1000000000)\n",
    "    testresult_scores = {d[\"section\"]: len(d[\"correct\"]) / (len(d[\"correct\"]) + len(d[\"incorrect\"])) for d in testresults[1]}\n",
    "    for measure, score in testresult_scores.items():\n",
    "        print(\"{}: {:.3f}\".format(measure, score))\n",
    "    return testresult_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v2 = Word2Vec.load(\"gutenberg_w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_w2v(w2v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
